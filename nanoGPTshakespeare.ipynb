{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM55loDnOa+URQHdicRzfdG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eniompw/nanoGPTshakespeare/blob/main/nanoGPTshakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zvPA9fIty5c",
        "outputId": "4c7c0c0c-4b5c-4b4c-e1ff-cc118281c828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 329, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 329 (delta 15), reused 29 (delta 13), pack-reused 288\u001b[K\n",
            "Receiving objects: 100% (329/329), 604.08 KiB | 21.57 MiB/s, done.\n",
            "Resolving deltas: 100% (183/183), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cs9c2P1fvRn",
        "outputId": "fca35fb0-840d-4533-ab72-1855e2963253"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blobfile>=2\n",
            "  Downloading blobfile-2.0.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.26.0\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex~=3.8\n",
            "  Downloading pycryptodomex-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken) (4.9.2)\n",
            "Collecting urllib3<3,>=1.25.3\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
            "Installing collected packages: tokenizers, urllib3, pycryptodomex, requests, blobfile, tiktoken, huggingface-hub, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "Successfully installed blobfile-2.0.1 huggingface-hub-0.12.0 pycryptodomex-3.16.0 requests-2.28.2 tiktoken-0.1.2 tokenizers-0.13.2 transformers-4.26.0 urllib3-1.26.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/nanoGPT/data/shakespeare/ && python prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxoMQ8IMt8Bu",
        "outputId": "d600d83a-4802-4e23-8aa6-8933e324b2db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/nanoGPT/ && python train.py --dataset=shakespeare --n_layer=4 --n_head=4 --n_embd=64 --compile=False --eval_iters=1 --block_size=64 --batch_size=8 --init_from=gpt2-medium --dtype=float32 --eval_interval=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBXoH_7tunr6",
        "outputId": "aa388680-56da-4490-fadb-0bcdfbd57eb1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dataset = shakespeare\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 64\n",
            "Overriding: compile = False\n",
            "Overriding: eval_iters = 1\n",
            "Overriding: block_size = 64\n",
            "Overriding: batch_size = 8\n",
            "Overriding: init_from = gpt2-medium\n",
            "Overriding: dtype = float32\n",
            "Overriding: eval_interval = 100\n",
            "vocab_size not found in data/shakespeare/meta.pkl, using GPT-2 default of 50257\n",
            "Initializing from OpenAI GPT-2 weights: gpt2-medium\n",
            "loading weights from pretrained gpt: gpt2-medium\n",
            "number of parameters: 354.82M\n",
            "Downloading (…)lve/main/config.json: 100% 718/718 [00:00<00:00, 293kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 1.52G/1.52G [00:05<00:00, 261MB/s]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 50.5kB/s]\n",
            "step 0: train loss 4.7863, val loss 4.7258\n",
            "iter 0: loss 4.3751, time 3219.25ms\n",
            "iter 1: loss 4.8796, time 455.09ms\n",
            "iter 2: loss 4.6159, time 462.35ms\n",
            "iter 3: loss 4.5771, time 458.44ms\n",
            "iter 4: loss 4.4890, time 460.53ms\n",
            "iter 5: loss 4.3868, time 460.69ms\n",
            "iter 6: loss 4.3343, time 457.85ms\n",
            "iter 7: loss 4.5887, time 462.18ms\n",
            "iter 8: loss 4.5480, time 462.74ms\n",
            "iter 9: loss 4.0378, time 461.63ms\n",
            "iter 10: loss 4.3881, time 462.25ms\n",
            "iter 11: loss 4.4789, time 461.90ms\n",
            "iter 12: loss 4.3718, time 464.49ms\n",
            "iter 13: loss 4.4185, time 467.46ms\n",
            "iter 14: loss 4.2711, time 462.65ms\n",
            "iter 15: loss 4.5385, time 464.94ms\n",
            "iter 16: loss 4.4874, time 462.05ms\n",
            "iter 17: loss 4.2802, time 462.86ms\n",
            "iter 18: loss 4.2268, time 460.98ms\n",
            "iter 19: loss 3.9693, time 465.20ms\n",
            "iter 20: loss 4.2491, time 462.06ms\n",
            "iter 21: loss 4.1851, time 464.69ms\n",
            "iter 22: loss 4.2358, time 465.71ms\n",
            "iter 23: loss 4.3490, time 464.89ms\n",
            "iter 24: loss 4.0285, time 465.60ms\n",
            "iter 25: loss 3.8456, time 466.27ms\n",
            "iter 26: loss 4.1123, time 467.69ms\n",
            "iter 27: loss 3.8622, time 467.80ms\n",
            "iter 28: loss 3.4621, time 466.61ms\n",
            "iter 29: loss 3.9084, time 464.22ms\n",
            "iter 30: loss 4.1280, time 465.91ms\n",
            "iter 31: loss 4.0925, time 473.28ms\n",
            "iter 32: loss 3.8752, time 468.40ms\n",
            "iter 33: loss 4.0292, time 466.29ms\n",
            "iter 34: loss 3.8251, time 466.64ms\n",
            "iter 35: loss 3.9359, time 465.50ms\n",
            "iter 36: loss 3.6502, time 469.65ms\n",
            "iter 37: loss 3.6149, time 471.38ms\n",
            "iter 38: loss 4.2268, time 467.94ms\n",
            "iter 39: loss 3.8090, time 469.94ms\n",
            "iter 40: loss 3.9263, time 469.41ms\n",
            "iter 41: loss 3.7053, time 468.84ms\n",
            "iter 42: loss 3.5158, time 468.62ms\n",
            "iter 43: loss 3.6048, time 472.46ms\n",
            "iter 44: loss 3.7286, time 472.41ms\n",
            "iter 45: loss 3.7108, time 473.59ms\n",
            "iter 46: loss 3.6626, time 468.59ms\n",
            "iter 47: loss 3.5424, time 471.66ms\n",
            "iter 48: loss 3.5136, time 465.65ms\n",
            "iter 49: loss 3.4387, time 474.23ms\n",
            "iter 50: loss 3.5708, time 471.46ms\n",
            "iter 51: loss 3.7197, time 469.57ms\n",
            "iter 52: loss 3.6527, time 474.16ms\n",
            "iter 53: loss 3.9186, time 473.05ms\n",
            "iter 54: loss 3.7257, time 473.08ms\n",
            "iter 55: loss 4.0644, time 477.07ms\n",
            "iter 56: loss 3.6074, time 474.09ms\n",
            "iter 57: loss 3.6357, time 477.71ms\n",
            "iter 58: loss 3.8245, time 475.52ms\n",
            "iter 59: loss 3.3272, time 471.88ms\n",
            "iter 60: loss 3.4787, time 480.58ms\n",
            "iter 61: loss 3.8351, time 475.59ms\n",
            "iter 62: loss 3.5186, time 474.70ms\n",
            "iter 63: loss 3.8848, time 475.17ms\n",
            "iter 64: loss 3.4690, time 479.14ms\n",
            "iter 65: loss 3.8428, time 476.47ms\n",
            "iter 66: loss 3.7064, time 478.86ms\n",
            "iter 67: loss 3.9678, time 475.26ms\n",
            "iter 68: loss 3.6935, time 478.59ms\n",
            "iter 69: loss 3.5062, time 477.50ms\n",
            "iter 70: loss 3.3990, time 480.11ms\n",
            "iter 71: loss 4.0587, time 475.40ms\n",
            "iter 72: loss 3.7813, time 475.33ms\n",
            "iter 73: loss 3.4703, time 476.03ms\n",
            "iter 74: loss 3.3938, time 475.72ms\n",
            "iter 75: loss 3.5969, time 476.12ms\n",
            "iter 76: loss 3.8593, time 475.79ms\n",
            "iter 77: loss 3.5130, time 475.16ms\n",
            "iter 78: loss 3.6676, time 472.57ms\n",
            "iter 79: loss 3.4747, time 473.39ms\n",
            "iter 80: loss 3.6780, time 474.55ms\n",
            "iter 81: loss 3.6521, time 472.49ms\n",
            "iter 82: loss 3.7972, time 476.34ms\n",
            "iter 83: loss 3.6321, time 471.87ms\n",
            "iter 84: loss 3.8470, time 470.68ms\n",
            "iter 85: loss 3.8614, time 471.36ms\n",
            "iter 86: loss 4.0675, time 470.18ms\n",
            "iter 87: loss 3.6930, time 470.89ms\n",
            "iter 88: loss 3.5007, time 469.55ms\n",
            "iter 89: loss 3.4203, time 470.91ms\n",
            "iter 90: loss 3.6806, time 472.20ms\n",
            "iter 91: loss 3.5445, time 469.92ms\n",
            "iter 92: loss 3.5920, time 468.44ms\n",
            "iter 93: loss 3.3506, time 470.73ms\n",
            "iter 94: loss 3.4784, time 472.20ms\n",
            "iter 95: loss 3.5231, time 475.62ms\n",
            "iter 96: loss 3.2894, time 471.59ms\n",
            "iter 97: loss 3.7451, time 466.08ms\n",
            "iter 98: loss 3.1286, time 466.88ms\n",
            "iter 99: loss 3.8842, time 467.05ms\n",
            "step 100: train loss 3.5147, val loss 3.5929\n",
            "saving checkpoint to out\n",
            "iter 100: loss 3.4603, time 17472.17ms\n",
            "iter 101: loss 3.6442, time 457.27ms\n",
            "iter 102: loss 3.6059, time 460.80ms\n",
            "iter 103: loss 3.4354, time 455.74ms\n",
            "iter 104: loss 3.4543, time 456.94ms\n",
            "iter 105: loss 3.6931, time 455.50ms\n",
            "iter 106: loss 3.3964, time 457.84ms\n",
            "iter 107: loss 3.5072, time 455.61ms\n",
            "iter 108: loss 3.6904, time 456.82ms\n",
            "iter 109: loss 3.5124, time 460.52ms\n",
            "iter 110: loss 3.1961, time 458.97ms\n",
            "iter 111: loss 3.5240, time 458.41ms\n",
            "iter 112: loss 3.6212, time 458.77ms\n",
            "iter 113: loss 3.7590, time 458.08ms\n",
            "iter 114: loss 3.3505, time 459.41ms\n",
            "iter 115: loss 3.6613, time 462.07ms\n",
            "iter 116: loss 3.9024, time 460.41ms\n",
            "iter 117: loss 3.4306, time 459.72ms\n",
            "iter 118: loss 3.7029, time 460.63ms\n",
            "iter 119: loss 3.6041, time 460.47ms\n",
            "iter 120: loss 3.5621, time 461.11ms\n",
            "iter 121: loss 3.5437, time 462.14ms\n",
            "iter 122: loss 3.3333, time 462.98ms\n",
            "iter 123: loss 3.4793, time 466.10ms\n",
            "iter 124: loss 3.6366, time 463.96ms\n",
            "iter 125: loss 3.6254, time 462.31ms\n",
            "iter 126: loss 3.2299, time 462.66ms\n",
            "iter 127: loss 3.4745, time 463.17ms\n",
            "iter 128: loss 3.4210, time 466.69ms\n",
            "iter 129: loss 3.4956, time 464.79ms\n",
            "iter 130: loss 3.5371, time 464.25ms\n",
            "iter 131: loss 2.9929, time 465.10ms\n",
            "iter 132: loss 3.6454, time 464.76ms\n",
            "iter 133: loss 3.6450, time 462.93ms\n",
            "iter 134: loss 3.9439, time 465.82ms\n",
            "iter 135: loss 3.5233, time 462.94ms\n",
            "iter 136: loss 3.1095, time 467.36ms\n",
            "iter 137: loss 3.6684, time 464.65ms\n",
            "iter 138: loss 3.0958, time 465.60ms\n",
            "iter 139: loss 3.4726, time 465.20ms\n",
            "iter 140: loss 3.4625, time 469.45ms\n",
            "iter 141: loss 3.9043, time 468.52ms\n",
            "iter 142: loss 3.7469, time 468.99ms\n",
            "iter 143: loss 3.5295, time 465.96ms\n",
            "iter 144: loss 3.0170, time 467.35ms\n",
            "iter 145: loss 3.6014, time 466.48ms\n",
            "iter 146: loss 3.4545, time 469.75ms\n",
            "iter 147: loss 3.5588, time 467.79ms\n",
            "iter 148: loss 3.6883, time 469.89ms\n",
            "iter 149: loss 3.5881, time 472.57ms\n",
            "iter 150: loss 3.7033, time 469.92ms\n",
            "iter 151: loss 3.6237, time 468.09ms\n",
            "iter 152: loss 3.4715, time 468.52ms\n",
            "iter 153: loss 3.3437, time 469.21ms\n",
            "iter 154: loss 3.3072, time 468.70ms\n",
            "iter 155: loss 3.0363, time 467.95ms\n",
            "iter 156: loss 3.3393, time 467.99ms\n",
            "iter 157: loss 3.8320, time 469.29ms\n",
            "iter 158: loss 3.1689, time 475.06ms\n",
            "iter 159: loss 3.3948, time 470.82ms\n",
            "iter 160: loss 3.4444, time 470.28ms\n",
            "iter 161: loss 3.4305, time 471.01ms\n",
            "iter 162: loss 3.1990, time 471.53ms\n",
            "iter 163: loss 3.3369, time 473.92ms\n",
            "iter 164: loss 3.4247, time 471.24ms\n",
            "iter 165: loss 3.5615, time 472.77ms\n",
            "iter 166: loss 3.2823, time 472.44ms\n",
            "iter 167: loss 3.4180, time 477.93ms\n",
            "iter 168: loss 3.5307, time 477.26ms\n",
            "iter 169: loss 3.4644, time 474.05ms\n",
            "iter 170: loss 3.4496, time 477.87ms\n",
            "iter 171: loss 3.1186, time 475.64ms\n",
            "iter 172: loss 3.5112, time 475.88ms\n",
            "iter 173: loss 3.1550, time 478.53ms\n",
            "iter 174: loss 3.4860, time 478.80ms\n",
            "iter 175: loss 3.6643, time 479.01ms\n",
            "iter 176: loss 3.4139, time 478.05ms\n",
            "iter 177: loss 3.0439, time 474.77ms\n",
            "iter 178: loss 3.5178, time 477.17ms\n",
            "iter 179: loss 3.3048, time 478.43ms\n",
            "iter 180: loss 3.6000, time 477.92ms\n",
            "iter 181: loss 3.3371, time 475.97ms\n",
            "iter 182: loss 3.3432, time 475.66ms\n",
            "iter 183: loss 3.3900, time 475.76ms\n",
            "iter 184: loss 3.5220, time 472.56ms\n",
            "iter 185: loss 3.3622, time 474.59ms\n",
            "iter 186: loss 3.2535, time 477.42ms\n",
            "iter 187: loss 3.1951, time 475.54ms\n",
            "iter 188: loss 3.2093, time 475.82ms\n",
            "iter 189: loss 3.3053, time 477.24ms\n",
            "iter 190: loss 3.1231, time 474.95ms\n",
            "iter 191: loss 3.2550, time 472.48ms\n",
            "iter 192: loss 3.1425, time 472.41ms\n",
            "iter 193: loss 3.6947, time 471.98ms\n",
            "iter 194: loss 3.0431, time 472.64ms\n",
            "iter 195: loss 3.1655, time 474.84ms\n",
            "iter 196: loss 3.5951, time 472.82ms\n",
            "iter 197: loss 3.1690, time 470.27ms\n",
            "iter 198: loss 3.3027, time 472.06ms\n",
            "iter 199: loss 3.3595, time 473.45ms\n",
            "step 200: train loss 3.1736, val loss 3.2831\n",
            "saving checkpoint to out\n",
            "iter 200: loss 3.6633, time 17668.42ms\n",
            "iter 201: loss 3.8141, time 456.46ms\n",
            "iter 202: loss 3.1094, time 465.91ms\n",
            "iter 203: loss 3.2498, time 458.87ms\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 273, in <module>\n",
            "    optimizer.step()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\", line 140, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py\", line 162, in step\n",
            "    adamw(params_with_grad,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py\", line 219, in adamw\n",
            "    func(params,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adamw.py\", line 273, in _single_tensor_adamw\n",
            "    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/nanoGPT && python sample.py --dtype=float32 --num_samples=5 --max_new_tokens=5 --start=\"to be\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOjpslgu6g4Y",
        "outputId": "9214b0e0-86d4-41fb-f8a3-ef8a4c18b6dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dtype = float32\n",
            "Overriding: num_samples = 5\n",
            "Overriding: max_new_tokens = 5\n",
            "Overriding: start = to be\n",
            "number of parameters: 353.84M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "to be more fair,\n",
            "To\n",
            "---------------\n",
            "to be a true friend?\n",
            "\n",
            "---------------\n",
            "to be for so doing\n",
            "And\n",
            "---------------\n",
            "to be with him;\n",
            "And\n",
            "---------------\n",
            "to be,\n",
            "For my father\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}