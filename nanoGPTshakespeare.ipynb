{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5bHbeGdhlRc1Wq8v6sy5L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eniompw/nanoGPTshakespeare/blob/main/nanoGPTshakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zvPA9fIty5c",
        "outputId": "cf2ec629-4579-474b-e3b9-33c8acc1dfb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 384, done.\u001b[K\n",
            "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 384 (delta 149), reused 137 (delta 132), pack-reused 187\u001b[K\n",
            "Receiving objects: 100% (384/384), 701.10 KiB | 16.30 MiB/s, done.\n",
            "Resolving deltas: 100% (227/227), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT\n",
        "#!cd ./nanoGPT && git checkout 001c1e7            # before bias bug\n",
        "#!cd ./nanoGPT && git reset --hard origin/master  # most recent\n",
        "#!cd ./nanoGPT && git log -1                      # which version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cs9c2P1fvRn",
        "outputId": "1aee4c21-cc2e-4cf4-8780-6743e39a7d03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2022.6.2)\n",
            "Collecting requests>=2.26.0\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blobfile>=2\n",
            "  Downloading blobfile-2.0.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting pycryptodomex~=3.8\n",
            "  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken) (4.9.2)\n",
            "Collecting urllib3<3,>=1.25.3\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
            "Installing collected packages: tokenizers, urllib3, pycryptodomex, requests, blobfile, tiktoken, huggingface-hub, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "Successfully installed blobfile-2.0.1 huggingface-hub-0.12.0 pycryptodomex-3.17 requests-2.28.2 tiktoken-0.1.2 tokenizers-0.13.2 transformers-4.26.0 urllib3-1.26.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT/data/shakespeare/ && python prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxoMQ8IMt8Bu",
        "outputId": "42b3aa6a-e0c0-4651-fc37-398673524388"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT/ && python train.py --dtype=float32 --dataset=shakespeare --compile=False --n_layer=4 --n_head=4 --n_embd=64 --block_size=64 --batch_size=8 --init_from=gpt2 --eval_interval=100 --eval_iters=100 --max_iters=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBXoH_7tunr6",
        "outputId": "97a6c10d-cfe2-4def-d099-4067e8cd2159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dtype = float32\n",
            "Overriding: dataset = shakespeare\n",
            "Overriding: compile = False\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 64\n",
            "Overriding: block_size = 64\n",
            "Overriding: batch_size = 8\n",
            "Overriding: init_from = gpt2\n",
            "Overriding: eval_interval = 100\n",
            "Overriding: eval_iters = 100\n",
            "Overriding: max_iters = 100\n",
            "vocab_size not found in data/shakespeare/meta.pkl, using GPT-2 default of 50257\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "number of parameters: 124.44M\n",
            "step 0: train loss 4.9280, val loss 4.8530\n",
            "iter 0: loss 4.8380, time 9469.94ms\n",
            "iter 1: loss 4.9759, time 655.52ms\n",
            "iter 2: loss 5.1479, time 655.52ms\n",
            "iter 3: loss 4.7238, time 652.17ms\n",
            "iter 4: loss 4.9314, time 655.81ms\n",
            "iter 5: loss 5.2162, time 660.61ms\n",
            "iter 6: loss 4.9769, time 654.31ms\n",
            "iter 7: loss 4.7778, time 654.02ms\n",
            "iter 8: loss 5.2068, time 664.40ms\n",
            "iter 9: loss 5.2082, time 653.02ms\n",
            "iter 10: loss 4.6858, time 658.29ms\n",
            "iter 11: loss 4.7892, time 658.96ms\n",
            "iter 12: loss 4.3185, time 656.86ms\n",
            "iter 13: loss 4.7808, time 662.84ms\n",
            "iter 14: loss 4.7024, time 656.44ms\n",
            "iter 15: loss 4.4880, time 664.59ms\n",
            "iter 16: loss 4.8076, time 656.35ms\n",
            "iter 17: loss 4.3334, time 664.94ms\n",
            "iter 18: loss 4.6288, time 659.55ms\n",
            "iter 19: loss 4.3309, time 658.59ms\n",
            "iter 20: loss 4.3420, time 657.02ms\n",
            "iter 21: loss 4.6633, time 665.40ms\n",
            "iter 22: loss 4.5661, time 657.66ms\n",
            "iter 23: loss 4.6346, time 662.36ms\n",
            "iter 24: loss 4.1621, time 660.33ms\n",
            "iter 25: loss 4.3946, time 661.26ms\n",
            "iter 26: loss 4.5990, time 660.06ms\n",
            "iter 27: loss 4.3208, time 661.05ms\n",
            "iter 28: loss 4.0736, time 661.18ms\n",
            "iter 29: loss 4.2260, time 660.89ms\n",
            "iter 30: loss 3.7578, time 665.90ms\n",
            "iter 31: loss 4.3797, time 658.45ms\n",
            "iter 32: loss 4.1859, time 667.74ms\n",
            "iter 33: loss 4.2485, time 666.69ms\n",
            "iter 34: loss 3.9175, time 663.11ms\n",
            "iter 35: loss 4.0225, time 659.82ms\n",
            "iter 36: loss 4.2686, time 667.91ms\n",
            "iter 37: loss 3.9885, time 664.55ms\n",
            "iter 38: loss 4.1366, time 662.39ms\n",
            "iter 39: loss 4.1616, time 668.13ms\n",
            "iter 40: loss 4.0563, time 666.74ms\n",
            "iter 41: loss 4.3220, time 656.71ms\n",
            "iter 42: loss 3.7634, time 668.18ms\n",
            "iter 43: loss 4.1068, time 669.90ms\n",
            "iter 44: loss 3.8670, time 662.60ms\n",
            "iter 45: loss 4.2348, time 667.38ms\n",
            "iter 46: loss 3.7439, time 671.84ms\n",
            "iter 47: loss 4.1030, time 668.43ms\n",
            "iter 48: loss 3.6962, time 669.95ms\n",
            "iter 49: loss 3.8749, time 671.99ms\n",
            "iter 50: loss 4.0822, time 670.96ms\n",
            "iter 51: loss 4.0497, time 668.11ms\n",
            "iter 52: loss 4.1018, time 662.54ms\n",
            "iter 53: loss 4.2560, time 669.97ms\n",
            "iter 54: loss 4.1538, time 669.28ms\n",
            "iter 55: loss 3.8839, time 668.51ms\n",
            "iter 56: loss 4.1631, time 671.09ms\n",
            "iter 57: loss 4.2434, time 670.22ms\n",
            "iter 58: loss 4.2772, time 672.35ms\n",
            "iter 59: loss 4.2124, time 668.87ms\n",
            "iter 60: loss 4.1731, time 670.31ms\n",
            "iter 61: loss 3.7933, time 671.23ms\n",
            "iter 62: loss 4.0809, time 673.30ms\n",
            "iter 63: loss 3.8423, time 669.76ms\n",
            "iter 64: loss 4.0617, time 672.71ms\n",
            "iter 65: loss 4.0467, time 673.04ms\n",
            "iter 66: loss 3.7969, time 672.13ms\n",
            "iter 67: loss 4.0915, time 668.62ms\n",
            "iter 68: loss 3.8152, time 671.74ms\n",
            "iter 69: loss 3.9587, time 671.00ms\n",
            "iter 70: loss 3.7305, time 672.63ms\n",
            "iter 71: loss 3.7418, time 670.27ms\n",
            "iter 72: loss 4.0073, time 668.93ms\n",
            "iter 73: loss 3.6016, time 668.76ms\n",
            "iter 74: loss 3.8858, time 672.65ms\n",
            "iter 75: loss 3.5702, time 669.34ms\n",
            "iter 76: loss 3.8718, time 671.09ms\n",
            "iter 77: loss 3.7848, time 673.57ms\n",
            "iter 78: loss 3.8957, time 668.89ms\n",
            "iter 79: loss 3.8492, time 670.50ms\n",
            "iter 80: loss 3.6415, time 673.14ms\n",
            "iter 81: loss 4.0135, time 667.23ms\n",
            "iter 82: loss 3.6489, time 672.62ms\n",
            "iter 83: loss 4.1699, time 673.53ms\n",
            "iter 84: loss 3.4156, time 670.33ms\n",
            "iter 85: loss 3.6561, time 666.84ms\n",
            "iter 86: loss 3.5857, time 671.96ms\n",
            "iter 87: loss 3.4874, time 668.54ms\n",
            "iter 88: loss 3.9538, time 669.54ms\n",
            "iter 89: loss 3.5989, time 677.31ms\n",
            "iter 90: loss 3.4762, time 673.61ms\n",
            "iter 91: loss 3.5806, time 674.50ms\n",
            "iter 92: loss 3.2627, time 675.65ms\n",
            "iter 93: loss 3.4662, time 673.66ms\n",
            "iter 94: loss 4.0500, time 674.96ms\n",
            "iter 95: loss 4.2655, time 674.98ms\n",
            "iter 96: loss 3.5366, time 678.19ms\n",
            "iter 97: loss 3.9281, time 673.74ms\n",
            "iter 98: loss 3.5707, time 675.83ms\n",
            "iter 99: loss 3.8393, time 667.09ms\n",
            "step 100: train loss 3.6804, val loss 3.7526\n",
            "saving checkpoint to out\n",
            "iter 100: loss 4.2000, time 14997.69ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT && python sample.py --dtype=float32 --num_samples=5 --max_new_tokens=10 --start=\"to be\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOjpslgu6g4Y",
        "outputId": "0218d9ca-b344-4c79-a837-163a6d9cbf21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dtype = float32\n",
            "Overriding: num_samples = 5\n",
            "Overriding: max_new_tokens = 10\n",
            "Overriding: start = to be\n",
            "number of parameters: 123.70M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "to be more fair,\n",
            "Than they have weaned\n",
            "---------------\n",
            "to be made so?\n",
            "\n",
            "DUKE VINC\n",
            "---------------\n",
            "to be,\n",
            "For my father's sake, under the\n",
            "---------------\n",
            "to be an island, nor a desert;\n",
            "I have\n",
            "---------------\n",
            "to be a pretty woman for.\n",
            "\n",
            "HENRY\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}