{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1Xs6k37JMcuZ+JvbQVlAJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eniompw/nanoGPTshakespeare/blob/main/nanoGPTshakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zvPA9fIty5c",
        "outputId": "ab79d7d6-1e2f-40ef-9167-98d54f7341aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 390, done.\u001b[K\n",
            "remote: Counting objects: 100% (203/203), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 390 (delta 153), reused 142 (delta 135), pack-reused 187\u001b[K\n",
            "Receiving objects: 100% (390/390), 701.70 KiB | 16.32 MiB/s, done.\n",
            "Resolving deltas: 100% (231/231), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cs9c2P1fvRn",
        "outputId": "cd6b9754-f838-4ec5-f11b-b9f50019ab0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blobfile>=2\n",
            "  Downloading blobfile-2.0.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2022.6.2)\n",
            "Collecting requests>=2.26.0\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting pycryptodomex~=3.8\n",
            "  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken) (4.9.2)\n",
            "Collecting urllib3<3,>=1.25.3\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
            "Installing collected packages: tokenizers, urllib3, pycryptodomex, requests, blobfile, tiktoken, huggingface-hub, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "Successfully installed blobfile-2.0.1 huggingface-hub-0.12.0 pycryptodomex-3.17 requests-2.28.2 tiktoken-0.1.2 tokenizers-0.13.2 transformers-4.26.0 urllib3-1.26.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT/data/shakespeare/ && python prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxoMQ8IMt8Bu",
        "outputId": "c14d6645-3065-47b6-b0d2-33b9eec7539f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT/ && python train.py --dtype=float16 --dataset=shakespeare --compile=False --n_layer=4 --n_head=4 --n_embd=64 --block_size=64 --batch_size=8 --init_from=gpt2 --eval_interval=100 --eval_iters=100 --max_iters=100 --bias=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBXoH_7tunr6",
        "outputId": "1669306a-c157-4975-b41c-2e1725cd81f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dtype = float16\n",
            "Overriding: dataset = shakespeare\n",
            "Overriding: compile = False\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 64\n",
            "Overriding: block_size = 64\n",
            "Overriding: batch_size = 8\n",
            "Overriding: init_from = gpt2\n",
            "Overriding: eval_interval = 100\n",
            "Overriding: eval_iters = 100\n",
            "Overriding: max_iters = 100\n",
            "Overriding: bias = True\n",
            "vocab_size not found in data/shakespeare/meta.pkl, using GPT-2 default of 50257\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "number of parameters: 124.44M\n",
            "Downloading (…)lve/main/config.json: 100% 665/665 [00:00<00:00, 274kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 548M/548M [00:06<00:00, 80.7MB/s]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 50.1kB/s]\n",
            "Initializing Gradient Scaler to account for dtype: float16\n",
            "step 0: train loss 4.9281, val loss 4.8530\n",
            "iter 0: loss 4.6113, time 7179.94ms\n",
            "iter 1: loss 4.9382, time 66.19ms\n",
            "iter 2: loss 5.1775, time 66.38ms\n",
            "iter 3: loss 4.8605, time 107.92ms\n",
            "iter 4: loss 4.8345, time 103.63ms\n",
            "iter 5: loss 4.9391, time 112.49ms\n",
            "iter 6: loss 4.9854, time 112.04ms\n",
            "iter 7: loss 4.7461, time 112.52ms\n",
            "iter 8: loss 4.9394, time 112.10ms\n",
            "iter 9: loss 4.8809, time 112.67ms\n",
            "iter 10: loss 4.8848, time 112.85ms\n",
            "iter 11: loss 4.8600, time 115.94ms\n",
            "iter 12: loss 5.0236, time 112.90ms\n",
            "iter 13: loss 4.9902, time 112.29ms\n",
            "iter 14: loss 4.8653, time 112.73ms\n",
            "iter 15: loss 4.4904, time 112.89ms\n",
            "iter 16: loss 4.5945, time 112.35ms\n",
            "iter 17: loss 4.8103, time 113.08ms\n",
            "iter 18: loss 4.2409, time 112.44ms\n",
            "iter 19: loss 4.4129, time 112.77ms\n",
            "iter 20: loss 4.8594, time 116.04ms\n",
            "iter 21: loss 4.3959, time 112.69ms\n",
            "iter 22: loss 4.4473, time 112.44ms\n",
            "iter 23: loss 4.5330, time 111.96ms\n",
            "iter 24: loss 4.3481, time 112.29ms\n",
            "iter 25: loss 4.0286, time 90.83ms\n",
            "iter 26: loss 4.6682, time 88.59ms\n",
            "iter 27: loss 4.2914, time 112.25ms\n",
            "iter 28: loss 4.0129, time 112.62ms\n",
            "iter 29: loss 4.4661, time 112.15ms\n",
            "iter 30: loss 4.3397, time 113.79ms\n",
            "iter 31: loss 4.6000, time 115.07ms\n",
            "iter 32: loss 4.2938, time 112.80ms\n",
            "iter 33: loss 4.1292, time 112.53ms\n",
            "iter 34: loss 4.3622, time 112.26ms\n",
            "iter 35: loss 4.0613, time 112.81ms\n",
            "iter 36: loss 4.2975, time 112.06ms\n",
            "iter 37: loss 4.4380, time 112.48ms\n",
            "iter 38: loss 3.8293, time 112.92ms\n",
            "iter 39: loss 4.1677, time 113.28ms\n",
            "iter 40: loss 4.2013, time 114.36ms\n",
            "iter 41: loss 4.3101, time 112.12ms\n",
            "iter 42: loss 4.1872, time 113.97ms\n",
            "iter 43: loss 4.2697, time 111.00ms\n",
            "iter 44: loss 4.3691, time 113.92ms\n",
            "iter 45: loss 4.3420, time 114.95ms\n",
            "iter 46: loss 3.9258, time 112.96ms\n",
            "iter 47: loss 4.2580, time 112.75ms\n",
            "iter 48: loss 4.2167, time 113.17ms\n",
            "iter 49: loss 4.4133, time 113.26ms\n",
            "iter 50: loss 4.2097, time 112.98ms\n",
            "iter 51: loss 3.8410, time 111.97ms\n",
            "iter 52: loss 4.1512, time 112.46ms\n",
            "iter 53: loss 4.0043, time 112.89ms\n",
            "iter 54: loss 3.8105, time 112.40ms\n",
            "iter 55: loss 4.1907, time 113.41ms\n",
            "iter 56: loss 3.7281, time 112.44ms\n",
            "iter 57: loss 4.0199, time 117.70ms\n",
            "iter 58: loss 3.6415, time 112.74ms\n",
            "iter 59: loss 3.9435, time 112.43ms\n",
            "iter 60: loss 4.3955, time 115.91ms\n",
            "iter 61: loss 4.1025, time 113.33ms\n",
            "iter 62: loss 3.8886, time 113.33ms\n",
            "iter 63: loss 3.7706, time 112.49ms\n",
            "iter 64: loss 3.5564, time 112.43ms\n",
            "iter 65: loss 3.8229, time 118.19ms\n",
            "iter 66: loss 3.6995, time 112.53ms\n",
            "iter 67: loss 4.1791, time 112.50ms\n",
            "iter 68: loss 4.0406, time 112.18ms\n",
            "iter 69: loss 3.8907, time 112.72ms\n",
            "iter 70: loss 3.8313, time 112.10ms\n",
            "iter 71: loss 3.9515, time 112.60ms\n",
            "iter 72: loss 3.8810, time 114.14ms\n",
            "iter 73: loss 3.7287, time 113.94ms\n",
            "iter 74: loss 3.8287, time 119.80ms\n",
            "iter 75: loss 3.8920, time 114.11ms\n",
            "iter 76: loss 3.8344, time 114.03ms\n",
            "iter 77: loss 3.9831, time 112.73ms\n",
            "iter 78: loss 3.6516, time 113.19ms\n",
            "iter 79: loss 3.8870, time 113.31ms\n",
            "iter 80: loss 3.5371, time 112.96ms\n",
            "iter 81: loss 3.8678, time 115.98ms\n",
            "iter 82: loss 3.9706, time 112.51ms\n",
            "iter 83: loss 3.9176, time 115.07ms\n",
            "iter 84: loss 4.2224, time 112.61ms\n",
            "iter 85: loss 3.7802, time 117.73ms\n",
            "iter 86: loss 4.0197, time 112.64ms\n",
            "iter 87: loss 4.0402, time 115.41ms\n",
            "iter 88: loss 3.7670, time 115.01ms\n",
            "iter 89: loss 3.6363, time 113.19ms\n",
            "iter 90: loss 3.4917, time 113.42ms\n",
            "iter 91: loss 4.0560, time 113.42ms\n",
            "iter 92: loss 3.9249, time 112.56ms\n",
            "iter 93: loss 3.6783, time 112.68ms\n",
            "iter 94: loss 4.0112, time 112.30ms\n",
            "iter 95: loss 3.4361, time 112.86ms\n",
            "iter 96: loss 3.9989, time 112.95ms\n",
            "iter 97: loss 3.9102, time 112.67ms\n",
            "iter 98: loss 3.6533, time 112.94ms\n",
            "iter 99: loss 3.4883, time 116.40ms\n",
            "step 100: train loss 3.8431, val loss 3.7902\n",
            "saving checkpoint to out\n",
            "iter 100: loss 3.2418, time 10011.31ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT && python sample.py --dtype=float16 --num_samples=5 --max_new_tokens=10 --start=\"to be\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOjpslgu6g4Y",
        "outputId": "50307dc5-4974-4565-924b-44bd235bcfe5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dtype = float16\n",
            "Overriding: num_samples = 5\n",
            "Overriding: max_new_tokens = 10\n",
            "Overriding: start = to be\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "WARNING: using slow attention, install PyTorch nightly for fast Flash Attention\n",
            "number of parameters: 123.70M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "to be more common, therefore, that they have wept\n",
            "---------------\n",
            "to be made so?\n",
            "\n",
            "DUKE DU\n",
            "---------------\n",
            "to be,\n",
            "For my father's sake, under the\n",
            "---------------\n",
            "to be an officer of the peace, and they are not\n",
            "---------------\n",
            "to be a man.\n",
            "\n",
            "LARRYDY\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}